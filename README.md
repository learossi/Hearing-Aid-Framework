# Hearing-Aid-Framework
A basic program to do hearing aid style audio processing. Built for STM-Nucleo boards. 
# Features
This is a framework to do basic hearing aid processing. Thus, is has several of the components that might be normal for the average hearing aid as well as a processing pipeline that enables programmers to package sequential tasks on the input data. 

The most important class is the FilterBank class which is a polyphase filter bank designed for use with cosine-modulated filters or other such design methods that allow the creation of uniform bandwidth filters from a prototype filter. One of the benefits of this polyphase setup is that it allows downsampling each of the frequency components into PPNcomp (polyphase network component) classes that are owned by the FilterBank class. Each of these can then be manipulated in order to create frequency selective modifications of the input signal. The primary class which does this is the Audiogram class. This class allows the user to specify an "Audiogram" at each frequency band (ie a desired level of gain at the center of the frequency band). The Audiogram class then linearly interpolates bewteen these desired gain levels to achieve a smooth(ish) transition between gain levels. This approach mimics the usual approach employed in programming hearing aids where audiologists will assess the desired gain at test frequencies in a hearing test depending on audibility and then interpolate between said frequencies in order to create a full gain profile. 

The framework also contains a couple utilities that are intended to be applied outside the filter bank representation of the data. The first is a compressor. Cochlear hearing loss (the most common type of hearing loss among the elderly) is often accompanied by problems with loudness recruitment (ie, the range of volume from which speech becomes intelligible to painfully loud is much narrower than in those with healthy hearing). Compressors are a way to mitigate this problem by capping the volume below a certain decible level and by pushing lower sounds levels into audible ranges (it compresses the range of volume). The obvious to do this would be to compose some sort of (possibly piecewise) affine transform of the volume level with a hard ceiling on the volume, but sudden changes in volume can give important clues to listeners about the importance of audio cues. So, modern compression designs choose to emulate characteristics of analog software and include an attack and release time as well as the parameters that affect the aforementioned gain levels. This freamework includes a compressor which allows users to specify a compresion ratio as well as attack and release times. 

Another utility included in the framework is a relatively simple adaptive noise canceller. Adaptive noise cancellers are adaptive filters (usually some variant on a Weiner filter) that try to emulate the feedback path of some control system in order to cancel out the feedback. The mathematical proof of the convergence for many of these types of filters is rather involved, but it has been shown that these types of filters converge in the mean to the true feedback path following a reasonable update strategy (like gradient descent). The practical realities of hearing aids make the implementation of these types of filters slightly more complicated than they would be usually. In fact, high autocorrelation like we usually observe in audio applications tends to impede convergence due to severe ill conditioning of the update. In order to ameliorate this, the inputs to the filter are delayed a few ms to decorrelate the input of the hearing aid from the filter input. The cost paid here is that the filter is unable to model the feedback which occurs during the delay. Nevertheless, adaptive filters can be used to cancel feedback which allows operators to push the gain of the system higher without risking instability - a common desire in hearing aids where volume is the main barrier to audibility. The adaptive filter in the framework is akin to the one just described. It assumes the feedback path is FIR and allows the user to set the order of the feedback path as well as the desired decorrelation delay. The update method is the LMS algorithm which is basically a fancy way of saying that it minimizes the L2 loss using gradient descent.  

# API
The main API is the Filter_Pipeline class. This class allows the user to specify an input/output array for each epoch and the operations to be done on said data. Before any operations are done, the programmer must push a sequential list of tasks (instantiated as std::function<void()> objects) to a vector contained in the Filter_Pipeline that define the operations that must be done to the data before it is outputted. The example included as of now uses lambdas to package functions with function calls to retrieve any relevant arguments but the only requirement is that whatever data type you use must be convertible to std::function which is leaves a good few options. Filter_Pipeline also contains a pointer (non-owning) to a FilterBank object which does the conversion to a filter bank frequency representation of the data. To this end, Filter_Pipeline comes with two custom tasks, initialize_filter_bank and terminate_filter_bank which call all the necessary functions in the filter bank pointed to that convert the data to that frequency representation. 

All the other utilities have their own APIs that depend on the specific purpose of the class. You can read the source files if you want some more detail. The long and short of it is that they are all intended to have calls to their API packaged in tasks which are then executed by the pipeline whenever the execute_tasks function is called. 

While not strictly a component of the API, the Implementation_Constants file is key to modifying the behavior of the system. Essentially, this is where all the important system constants are such as numbers of samples per epoch, the downsampling rate, the number of filter banks, the complex number type, hooks for the fft function, etc. The entire idea behind hearing aids is that they're real time audio processing where system constants don't change, so that is enforced using what are essentially const globals in this file that govern how the rest of the system performs. This is probably the first place to look if you want to create a custom implementation of your own since it forces you to figure out what specifications actually work for your system. 

# Dependencies
The way the program is written now is to accept DMA input from an STM32-Nucleo board. The hardware functionality is regulated using the HAL supplied by STM. This is packaged as part of the STM32Cube package or as part of the IDE with the same name. The important main thing to set this up is to configure the configuration template file that comes with each of these to be congruent with your board and set it in the include path. Everything else should work out of the box. 

There's also a dependency on the CMSIS DSP package for the FFT function. However, there only place it's included in the code is in the FFT hook in the Implementation Constants file so it's pretty easy to sub out if you want to. 

I'm not packaging either of these here so you'll have to deal with that on your own if you want to. 

# Compilation
I compiled this using the Arm Compiler for Embedded projects. I think you can basically compile it on whatever you want as long as it supports your target output. The main dependencies in std I rely on are array, cmath, and functional which I'm pretty sure are all pre-C++11 features. 
